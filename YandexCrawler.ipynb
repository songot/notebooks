{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: virtualenv in /srv/conda/envs/notebook/lib/python3.7/site-packages (20.0.21)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.5.0)\n",
      "Requirement already satisfied: filelock<4,>=3.0.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (3.0.12)\n",
      "Requirement already satisfied: appdirs<2,>=1.4.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.4.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (0.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->virtualenv) (2.1.0)\n",
      "created virtual environment CPython3.7.6.final.0-64 in 406ms\n",
      "  creator CPython3Posix(dest=/home/jovyan/scrapers/yandex/env, clear=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/home/jovyan/.local/share/virtualenv/seed-app-data/v1.0.1)\n",
      "  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n",
      "Requirement already satisfied: scrapy in ./env/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in ./env/lib/python3.7/site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in ./env/lib/python3.7/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in ./env/lib/python3.7/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in ./env/lib/python3.7/site-packages (from scrapy) (20.3.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in ./env/lib/python3.7/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in ./env/lib/python3.7/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in ./env/lib/python3.7/site-packages (from scrapy) (4.5.1)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in ./env/lib/python3.7/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in ./env/lib/python3.7/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in ./env/lib/python3.7/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in ./env/lib/python3.7/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in ./env/lib/python3.7/site-packages (from scrapy) (2.9.2)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (46.4.0)\n",
      "Requirement already satisfied: six in ./env/lib/python3.7/site-packages (from protego>=0.1.15->scrapy) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in ./env/lib/python3.7/site-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
      "Requirement already satisfied: pyasn1 in ./env/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in ./env/lib/python3.7/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in ./env/lib/python3.7/site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.5 in ./env/lib/python3.7/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n",
      "Requirement already satisfied: pycparser in ./env/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.18.4\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./env/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Installing collected packages: python-dateutil, pytz, pandas\n",
      "Successfully installed pandas-1.0.4 python-dateutil-2.8.1 pytz-2020.1\n",
      "Requirement already satisfied: requests in ./env/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./env/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./env/lib/python3.7/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: scraperapi-sdk in ./env/lib/python3.7/site-packages (0.2.2)\n",
      "Error: scrapy.cfg already exists in /home/jovyan/scrapers/yandex/yandex_scraper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘scrapers/’: File exists\n",
      "mkdir: cannot create directory ‘scrapers/yandex’: File exists\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'mkdir scrapers/\\nmkdir scrapers/yandex\\ncd scrapers/yandex\\npython -m pip install virtualenv\\nvirtualenv env\\n. env/bin/activate\\npython -m pip install scrapy\\npython -m pip install numpy\\npython -m pip install pandas\\npython -m pip install requests\\npython -m pip install scraperapi-sdk\\nscrapy startproject yandex_scraper\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75c1fdbaf6d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'script'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mkdir scrapers/\\nmkdir scrapers/yandex\\ncd scrapers/yandex\\npython -m pip install virtualenv\\nvirtualenv env\\n. env/bin/activate\\npython -m pip install scrapy\\npython -m pip install numpy\\npython -m pip install pandas\\npython -m pip install requests\\npython -m pip install scraperapi-sdk\\nscrapy startproject yandex_scraper\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2350\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2352\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</srv/conda/envs/notebook/lib/python3.7/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'mkdir scrapers/\\nmkdir scrapers/yandex\\ncd scrapers/yandex\\npython -m pip install virtualenv\\nvirtualenv env\\n. env/bin/activate\\npython -m pip install scrapy\\npython -m pip install numpy\\npython -m pip install pandas\\npython -m pip install requests\\npython -m pip install scraperapi-sdk\\nscrapy startproject yandex_scraper\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "mkdir scrapers/\n",
    "mkdir scrapers/yandex\n",
    "cd scrapers/yandex\n",
    "python -m pip install virtualenv\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install scrapy\n",
    "python -m pip install numpy\n",
    "python -m pip install pandas\n",
    "python -m pip install requests\n",
    "python -m pip install scraperapi-sdk\n",
    "scrapy startproject yandex_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing yandex_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile yandex_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scraper_api import ScraperAPIClient\n",
    "\n",
    "# remember to install the library: pip install scraperapi-sdk\n",
    "#from scraper_api import ScraperAPIClient\n",
    "#client = ScraperAPIClient('YOURAPIKEY')\n",
    "#result = client.get(url = 'http://httpbin.org/ip').text\n",
    "#print(result);\n",
    "# Scrapy users can simply replace the urls in their start_urls and parse function\n",
    "# Note for Scrapy, you should not use DOWNLOAD_DELAY and\n",
    "# RANDOMIZE_DOWNLOAD_DELAY, these will lower your concurrency and are not\n",
    "# needed with our API\n",
    "\n",
    "# ...other scrapy setup code\n",
    "#start_urls =[client.scrapyGet(url = 'http://httpbin.org/ip')]\n",
    "#def parse(self, response):\n",
    "\n",
    "# ...your parsing logic here\n",
    "#yield scrapy.Request(client.scrapyGet(url = 'http://httpbin.org/ip'), self.parse)\n",
    "\n",
    "#\"СЕО оптимизация сайта\"; \"СЕО Сайта\"; \"СЕО продвижение\"; \"СЕО оптимизация и продвижение\"\n",
    "#'https://yandex.com/?q=%22СЕО оптимизация сайта%22%3B%20%22СЕО Сайта%22%3B%20%22СЕО продвижение%22%3B%20%22СЕО оптимизация и продвижение%22%3B%20'\n",
    "# organic__extralinks\n",
    "\n",
    "class YandexSpider(scrapy.Spider):\n",
    "\tname = 'yandex'\n",
    "\tclient = ScraperAPIClient('20195ff71f205bc83477c2cc97a76379')\n",
    "\t#start_urls = [client.scrapyGet(url = 'yandex.ru/search/?text=%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%A1%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22&rdrnd=662718&lr=213&redircnt=1590621126.1'), ]\n",
    "\tstart_urls = []\n",
    "\tsearch_engine='https://yandex.com/?q='\n",
    "\tkeys = ['СЕО оптимизация сайта', 'СЕО Сайта', 'СЕО продвижение', 'СЕО оптимизация и продвижение']\n",
    "\tsurl = search_engine\n",
    "\tfor key in keys:\n",
    "\t\tsurl = surl + '%22' + key + '%22%3B%20'\n",
    "\tstart_urls.append(client.scrapyGet(url=surl))\n",
    "\t\n",
    "\turl_data = np.array([])\n",
    "\turl_counter = 0\n",
    "\t\n",
    "\tdef parse(self, response):\n",
    "\t\tlinks = response.css('div.organic__extralinks').xpath('@data-bem').get()\n",
    "\t\t#links = response.css('b.organic__extralinks')\n",
    "\t\t#self.url_data = np.append(self.url_data, np.array(links))\n",
    "\t\t#self.url_counter+=len(links)\n",
    "\t\t\n",
    "\t\t#if self.url_counter < 1000:\n",
    "\t\t\t#next _page = response.css('')\n",
    "\t\t\t#yield response.follow(next_page, self.parse)\n",
    "\t\t\t\n",
    "\"\"\"\n",
    "here are more examples:\n",
    "https://24ho.ru/\n",
    "https://seo.analizsaita.online/\n",
    "https://pr-cy.ru/analysis/\n",
    "https://sitechecker.pro/ru/\n",
    "https://zen.yandex.ru/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash \n",
    "mv yandex_spider.py scrapers/yandex/yandex_scraper/yandex_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-30 05:28:05 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: yandex_scraper)\n",
      "2020-05-30 05:28:05 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.15.0-1064-azure-x86_64-with-debian-buster-sid\n",
      "2020-05-30 05:28:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-05-30 05:28:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'yandex_scraper',\n",
      " 'NEWSPIDER_MODULE': 'yandex_scraper.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['yandex_scraper.spiders']}\n",
      "2020-05-30 05:28:05 [scrapy.extensions.telnet] INFO: Telnet Password: bde41295d30aee0a\n",
      "2020-05-30 05:28:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-30 05:28:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-30 05:28:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-30 05:28:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-30 05:28:05 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-30 05:28:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-30 05:28:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-30 05:28:06 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://api.scraperapi.com/robots.txt> (referer: None)\n",
      "2020-05-30 05:28:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.com%2F%3Fq%3D%2522%D0%A1%D0%95%D0%9E+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D1%81%D0%B0%D0%B9%D1%82%D0%B0%2522%253B%2520%2522%D0%A1%D0%95%D0%9E+%D0%A1%D0%B0%D0%B9%D1%82%D0%B0%2522%253B%2520%2522%D0%A1%D0%95%D0%9E+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%2522%253B%2520%2522%D0%A1%D0%95%D0%9E+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D0%B8+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%2522%253B%2520&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: None)\n",
      "2020-05-30 05:28:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-30 05:28:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1034,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 53069,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 8.806012,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 30, 5, 28, 14, 724958),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 99475456,\n",
      " 'memusage/startup': 99475456,\n",
      " 'response_received_count': 2,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 5, 30, 5, 28, 5, 918946)}\n",
      "2020-05-30 05:28:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd yandex_scraper/\n",
    "scrapy crawl yandex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
