{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.0.21-py2.py3-none-any.whl (4.7 MB)\n",
      "Collecting appdirs<2,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.5.0)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.14.0)\n",
      "Collecting distlib<1,>=0.3.0\n",
      "  Downloading distlib-0.3.0.zip (571 kB)\n",
      "Collecting filelock<4,>=3.0.0\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->virtualenv) (2.1.0)\n",
      "Building wheels for collected packages: distlib\n",
      "  Building wheel for distlib (setup.py): started\n",
      "  Building wheel for distlib (setup.py): finished with status 'done'\n",
      "  Created wheel for distlib: filename=distlib-0.3.0-py3-none-any.whl size=340426 sha256=645380c0fb07d2341602c7625dbc5d6a1730e5d4a7570b56be8a9234e70bd873\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a2/19/da/a15d4e2bedf3062c739b190d5cb5b7b2ecfbccb6b0d93c861b\n",
      "Successfully built distlib\n",
      "Installing collected packages: appdirs, distlib, filelock, virtualenv\n",
      "Successfully installed appdirs-1.4.4 distlib-0.3.0 filelock-3.0.12 virtualenv-20.0.21\n",
      "created virtual environment CPython3.7.6.final.0-64 in 417ms\n",
      "  creator CPython3Posix(dest=/home/jovyan/scrapers/yandex/env, clear=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/home/jovyan/.local/share/virtualenv/seed-app-data/v1.0.1)\n",
      "  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.1.0-py2.py3-none-any.whl (239 kB)\n",
      "Collecting cryptography>=2.0\n",
      "  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-manylinux1_x86_64.whl (3.1 MB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.1.0-cp37-cp37m-manylinux2010_x86_64.whl (235 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pyOpenSSL>=16.2.0\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting lxml>=3.5.0\n",
      "  Downloading lxml-4.5.1-cp37-cp37m-manylinux1_x86_64.whl (5.5 MB)\n",
      "Collecting six>=1.4.1\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading cffi-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (400 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-19.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (46.4.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting idna>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py): started\n",
      "  Building wheel for protego (setup.py): finished with status 'done'\n",
      "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=e2ccbc30fc582842cbde47e70f9d6dbebf5ad7f580ff79bfb3a21c5605720e51\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=d28dd0cca800788f66a46c8fccf2dc4f3aacb68d0be75050c8e837d19e06bcb2\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: six, pycparser, cffi, cryptography, zope.interface, PyHamcrest, attrs, Automat, constantly, incremental, idna, hyperlink, Twisted, cssselect, w3lib, lxml, parsel, queuelib, pyasn1, pyasn1-modules, service-identity, pyOpenSSL, protego, PyDispatcher, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 attrs-19.3.0 cffi-1.14.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 idna-2.9 incremental-17.5.0 lxml-4.5.1 parsel-1.6.0 protego-0.1.16 pyOpenSSL-19.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 queuelib-1.5.0 scrapy-2.1.0 service-identity-18.1.0 six-1.15.0 w3lib-1.22.0 zope.interface-5.1.0\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.18.4\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./env/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Installing collected packages: python-dateutil, pytz, pandas\n",
      "Successfully installed pandas-1.0.4 python-dateutil-2.8.1 pytz-2020.1\n",
      "Collecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Installing collected packages: certifi, chardet, urllib3, requests\n",
      "Successfully installed certifi-2020.4.5.1 chardet-3.0.4 requests-2.23.0 urllib3-1.25.9\n",
      "Collecting scraperapi-sdk\n",
      "  Downloading scraperapi_sdk-0.2.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: scraperapi-sdk\n",
      "Successfully installed scraperapi-sdk-0.2.2\n",
      "New Scrapy project 'yandex_scraper', using template directory '/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/templates/project', created in:\n",
      "    /home/jovyan/scrapers/yandex/yandex_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd yandex_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "mkdir scrapers/\n",
    "mkdir scrapers/yandex\n",
    "cd scrapers/yandex\n",
    "python -m pip install virtualenv\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install scrapy\n",
    "python -m pip install numpy\n",
    "python -m pip install pandas\n",
    "python -m pip install requests\n",
    "python -m pip install scraperapi-sdk\n",
    "scrapy startproject yandex_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for yandex_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'yandex_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['yandex_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'yandex_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'yandex_scraper (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'yandex_scraper.pipelines.YandexScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/yandex/yandex_scraper/yandex_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing yandex_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile yandex_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scraper_api import ScraperAPIClient\n",
    "\n",
    "class YandexSpider(scrapy.Spider):\n",
    "    name = 'yandex'\n",
    "    client = ScraperAPIClient('20195ff71f205bc83477c2cc97a76379')\n",
    "    start_urls = [client.scrapyGet(url='https://yandex.ru/search/?text=%22введите%22%20%22url%22%20СЕО%20анализ%20сайта&lr=213')]\n",
    "    search_engine='https://yandex.ru'\n",
    "    #keys = ['СЕО оптимизация сайта', 'СЕО Сайта', 'СЕО продвижение', 'СЕО оптимизация и продвижение']\n",
    "    #surl = search_engine\n",
    "    #for key in keys: surl = surl + '%22' + key + '%22%3B%20'\n",
    "    \n",
    "    #start_urls.append(client.scrapyGet(url=surl))\n",
    "    url_data = np.array([])\n",
    "    url_counter = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        links =\\\n",
    "        response.css('div.organic__path a::attr(href)').getall()#xpath('@href').getall()\n",
    "        #print 'here we go'\n",
    "        for link in links:\n",
    "            self.url_data = np.append(self.url_data, link)\n",
    "            self.url_counter+=1\n",
    "        next_page = response.css('a.pager__item_kind_next::attr(href)').get()\n",
    "        if self.url_counter<1500:\n",
    "            yield scrapy.Request(self.client.scrapyGet(url = self.search_engine+next_page), self.parse)\n",
    "        \n",
    "        #links = response.css('b.organic__extralinks')\n",
    "        #self.url_data = np.append(self.url_data, np.array(links))\n",
    "        #self.url_counter+=len(links)\n",
    "        #if self.url_counter < 1000:\n",
    "        #next _page = response.css('')\n",
    "        #yield response.follow(next_page, self.parse)\n",
    "    def closed(self, reason):\n",
    "        print(self.url_counter)\n",
    "        raw = pd.DataFrame(data=self.url_data)\n",
    "        raw.to_csv(r'/home/jovyan/raw_url.csv', index = False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash \n",
    "mv yandex_spider.py scrapers/yandex/yandex_scraper/yandex_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 01:17:01 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: yandex_scraper)\n",
      "2020-05-31 01:17:01 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.14.138+-x86_64-with-debian-buster-sid\n",
      "2020-05-31 01:17:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-05-31 01:17:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'yandex_scraper',\n",
      " 'NEWSPIDER_MODULE': 'yandex_scraper.spiders',\n",
      " 'SPIDER_MODULES': ['yandex_scraper.spiders']}\n",
      "2020-05-31 01:17:01 [scrapy.extensions.telnet] INFO: Telnet Password: 6b1a7ec0ee577431\n",
      "2020-05-31 01:17:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-31 01:17:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-31 01:17:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-31 01:17:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-31 01:17:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-31 01:17:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-31 01:17:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-31 01:17:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%D0%B2%D0%B2%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%2522%2520%2522url%2522%2520%D0%A1%D0%95%D0%9E%2520%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%2520%D1%81%D0%B0%D0%B9%D1%82%D0%B0%26lr%3D213&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: None)\n",
      "2020-05-31 01:17:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D1&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%D0%B2%D0%B2%D0%B5%D0%B4%D0%B8%D1%82%D0%B5%2522%2520%2522url%2522%2520%D0%A1%D0%95%D0%9E%2520%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%2520%D1%81%D0%B0%D0%B9%D1%82%D0%B0%26lr%3D213&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D2&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D1&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D3&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D2&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D4%26rnd%3D38029&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D3&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D5%26rnd%3D25354&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D4%26rnd%3D38029&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D6%26rnd%3D15914&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D5%26rnd%3D25354&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D7%26rnd%3D9374&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D6%26rnd%3D15914&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D8%26rnd%3D80906&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D7%26rnd%3D9374&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D9%26rnd%3D10200&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D8%26rnd%3D80906&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D10%26rnd%3D19169&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D9%26rnd%3D10200&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D11%26rnd%3D40739&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D10%26rnd%3D19169&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D12%26rnd%3D67176&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D11%26rnd%3D40739&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:17:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D13%26rnd%3D32271&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D12%26rnd%3D67176&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:01 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 14 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-31 01:18:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D14%26rnd%3D55755&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D13%26rnd%3D32271&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D15%26rnd%3D89748&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D14%26rnd%3D55755&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D16%26rnd%3D14425&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D15%26rnd%3D89748&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D17%26rnd%3D29461&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D16%26rnd%3D14425&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D18%26rnd%3D48187&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D17%26rnd%3D29461&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:18:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D19%26rnd%3D70511&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D18%26rnd%3D48187&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D20%26rnd%3D54265&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D19%26rnd%3D70511&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:01 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 7 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-31 01:19:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D21%26rnd%3D48229&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D20%26rnd%3D54265&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D22%26rnd%3D56155&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D21%26rnd%3D48229&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D23%26rnd%3D39851&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D22%26rnd%3D56155&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D24%26rnd%3D15103&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D23%26rnd%3D39851&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "2020-05-31 01:19:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D24%26rnd%3D15103&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python> (referer: https://api.scraperapi.com/?url=https%3A%2F%2Fyandex.ru%2Fsearch%2F%3Ftext%3D%2522%25D0%25B2%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B8%25D1%2582%25D0%25B5%2522%2520%2522url%2522%2520%25D0%25A1%25D0%2595%25D0%259E%2520%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%2520%25D1%2581%25D0%25B0%25D0%25B9%25D1%2582%25D0%25B0%26lr%3D213%26p%3D23%26rnd%3D39851&api_key=20195ff71f205bc83477c2cc97a76379&scraper_sdk=python)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/utils/defer.py\", line 117, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/utils/python.py\", line 345, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/utils/python.py\", line 345, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/spidermiddlewares/referer.py\", line 338, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/core/spidermw.py\", line 64, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/jovyan/scrapers/yandex/yandex_scraper/yandex_scraper/spiders/yandex_spider.py\", line 28, in parse\n",
      "    yield scrapy.Request(self.client.scrapyGet(url = self.search_engine+next_page), self.parse)\n",
      "TypeError: can only concatenate str (not \"NoneType\") to str\n",
      "2020-05-31 01:19:31 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-31 01:19:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25497,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 1743048,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 150.000154,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 31, 1, 19, 31, 568815),\n",
      " 'log_count/DEBUG': 25,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 137740288,\n",
      " 'memusage/startup': 100151296,\n",
      " 'request_depth_max': 24,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'spider_exceptions/TypeError': 1,\n",
      " 'start_time': datetime.datetime(2020, 5, 31, 1, 17, 1, 568661)}\n",
      "2020-05-31 01:19:31 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd yandex_scraper/\n",
    "scrapy crawl yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URLs    547\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('raw_url.csv', names=['URLs'], index_col=False)\n",
    "duplicates_off = data.drop_duplicates()\n",
    "duplicates_off.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e56938c4c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ruby.db.elephantsql.com (ruby-01)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rdqotkgq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'postgres://rdqotkgq:xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S@ruby.db.elephantsql.com:5432/rdqotkgq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ama turtle, hello!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from contextlib import closing\n",
    "\n",
    "with closing(psycopg2.connect(dbname='ruby.db.elephantsql.com (ruby-01)', user='rdqotkgq', password='xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S', host='postgres://rdqotkgq:xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S@ruby.db.elephantsql.com:5432/rdqotkgq')) as conn:\n",
    "    print(\"ama turtle, hello!\")\n",
    "    #with conn.cursor() as cursor:\n",
    "        #cursor.execute('SELECT * FROM airport LIMIT 5')\n",
    "        #for row in cursor:\n",
    "            #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ssu: not found\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\"SEO анализ\" | \"СЕО анализ\" | \"анализ\" | \"бесплатный анализ\" | \"Бесплатный СЕО анализ\" | \"Бесплатный SEO анализ\") + \"сайта\" + \" онлайн\"\n",
    "# remember to install the library: pip install scraperapi-sdk\n",
    "#from scraper_api import ScraperAPIClient\n",
    "#client = ScraperAPIClient('YOURAPIKEY')\n",
    "#result = client.get(url = 'http://httpbin.org/ip').text\n",
    "#print(result);\n",
    "# Scrapy users can simply replace the urls in their start_urls and parse function\n",
    "# Note for Scrapy, you should not use DOWNLOAD_DELAY and\n",
    "# RANDOMIZE_DOWNLOAD_DELAY, these will lower your concurrency and are not\n",
    "# needed with our API\n",
    "\n",
    "# ...other scrapy setup code\n",
    "#start_urls =[client.scrapyGet(url = 'http://httpbin.org/ip')]\n",
    "#def parse(self, response):\n",
    "\n",
    "# ...your parsing logic here\n",
    "#yield scrapy.Request(client.scrapyGet(url = 'http://httpbin.org/ip'), self.parse)\n",
    "\n",
    "#СЕО оптимизация сайта СЕО Сайта СЕО продвижение СЕО оптимизация и продвижение\n",
    "#'https://yandex.com/?q=%22СЕО оптимизация сайта%22%3B%20%22СЕО Сайта%22%3B%20%22СЕО продвижение%22%3B%20%22СЕО оптимизация и продвижение%22%3B%20'\n",
    "# organic__extralinks\n",
    "\n",
    "\"\"\"\n",
    "here are more examples:\n",
    "https://24ho.ru/\n",
    "https://seo.analizsaita.online/\n",
    "https://pr-cy.ru/analysis/\n",
    "https://sitechecker.pro/ru/\n",
    "https://zen.yandex.ru/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#start_urls = [client.scrapyGet(url = 'yandex.ru/search/?text=%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%A1%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22&rdrnd=662718&lr=213&redircnt=1590621126.1'), ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
