{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.0.21-py2.py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: importlib-metadata<2,>=0.12; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.5.0)\n",
      "Collecting filelock<4,>=3.0.0\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting distlib<1,>=0.3.0\n",
      "  Downloading distlib-0.3.0.zip (571 kB)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from virtualenv) (1.14.0)\n",
      "Collecting appdirs<2,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata<2,>=0.12; python_version < \"3.8\"->virtualenv) (2.1.0)\n",
      "Building wheels for collected packages: distlib\n",
      "  Building wheel for distlib (setup.py): started\n",
      "  Building wheel for distlib (setup.py): finished with status 'done'\n",
      "  Created wheel for distlib: filename=distlib-0.3.0-py3-none-any.whl size=340426 sha256=23251ac94d260bdfe686a14d996a86fe31214b55a14e1a72e490bebf34ead8af\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a2/19/da/a15d4e2bedf3062c739b190d5cb5b7b2ecfbccb6b0d93c861b\n",
      "Successfully built distlib\n",
      "Installing collected packages: filelock, distlib, appdirs, virtualenv\n",
      "Successfully installed appdirs-1.4.4 distlib-0.3.0 filelock-3.0.12 virtualenv-20.0.21\n",
      "created virtual environment CPython3.7.6.final.0-64 in 543ms\n",
      "  creator CPython3Posix(dest=/home/jovyan/scrapers/yandex/env, clear=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/home/jovyan/.local/share/virtualenv/seed-app-data/v1.0.1)\n",
      "  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.1.0-py2.py3-none-any.whl (239 kB)\n",
      "Collecting lxml>=3.5.0\n",
      "  Downloading lxml-4.5.1-cp37-cp37m-manylinux1_x86_64.whl (5.5 MB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pyOpenSSL>=16.2.0\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cryptography>=2.0\n",
      "  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0-cp37-cp37m-manylinux1_x86_64.whl (3.1 MB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.1.0-cp37-cp37m-manylinux2010_x86_64.whl (235 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting six>=1.5.2\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading cffi-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (400 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-19.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.7/site-packages (from zope.interface>=4.1.3->scrapy) (46.4.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting idna>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py): started\n",
      "  Building wheel for protego (setup.py): finished with status 'done'\n",
      "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=c7f2e7748af6692745ebd86bf08adc9cd01ae7eb91305f40c15e2fa7add0b144\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ca/44/01/3592ccfbcfaee4ab297c4097e6e9dbe1c7697e3531a39877ab\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=4bd9a913ec17716c878c49f1354bac255d8a3757a2ac1ff422b5c9f400f978a4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/dc/d0/bf/0cc715c01fce0bace63b46283acf5cc630d5e5dbb4602c54e5\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: lxml, cssselect, six, pycparser, cffi, cryptography, pyOpenSSL, queuelib, protego, PyDispatcher, w3lib, parsel, idna, hyperlink, attrs, constantly, zope.interface, incremental, Automat, PyHamcrest, Twisted, pyasn1, pyasn1-modules, service-identity, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 attrs-19.3.0 cffi-1.14.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 idna-2.9 incremental-17.5.0 lxml-4.5.1 parsel-1.6.0 protego-0.1.16 pyOpenSSL-19.1.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 queuelib-1.5.0 scrapy-2.1.0 service-identity-18.1.0 six-1.15.0 w3lib-1.22.0 zope.interface-5.1.0\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.18.4\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./env/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Installing collected packages: python-dateutil, pytz, pandas\n",
      "Successfully installed pandas-1.0.4 python-dateutil-2.8.1 pytz-2020.1\n",
      "Collecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Installing collected packages: chardet, certifi, urllib3, requests\n",
      "Successfully installed certifi-2020.4.5.1 chardet-3.0.4 requests-2.23.0 urllib3-1.25.9\n",
      "Collecting scraperapi-sdk\n",
      "  Downloading scraperapi_sdk-0.2.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: scraperapi-sdk\n",
      "Successfully installed scraperapi-sdk-0.2.2\n",
      "New Scrapy project 'yandex_scraper', using template directory '/home/jovyan/scrapers/yandex/env/lib/python3.7/site-packages/scrapy/templates/project', created in:\n",
      "    /home/jovyan/scrapers/yandex/yandex_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd yandex_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "mkdir scrapers/\n",
    "mkdir scrapers/yandex\n",
    "cd scrapers/yandex\n",
    "python -m pip install virtualenv\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install scrapy\n",
    "python -m pip install numpy\n",
    "python -m pip install pandas\n",
    "python -m pip install requests\n",
    "python -m pip install scraperapi-sdk\n",
    "scrapy startproject yandex_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for yandex_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'yandex_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['yandex_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'yandex_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'yandex_scraper (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'yandex_scraper.pipelines.YandexScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/yandex/yandex_scraper/yandex_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting yandex_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile yandex_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scraper_api import ScraperAPIClient\n",
    "\n",
    "class YandexSpider(scrapy.Spider):\n",
    "    name = 'yandex'\n",
    "    client = ScraperAPIClient('20195ff71f205bc83477c2cc97a76379')\n",
    "    start_urls = [client.scrapyGet(url='https://yandex.ru/search/?text=СЕО%20оптимизация%20сайта%20СЕО%20Сайта%20СЕО%20продвижение%20СЕО%20оптимизация%20и%20продвижение&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=бесплатный%20анализ%20сайта%20%22Введите%22%20%22url%22&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=сайт%20url%20анализ&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?lr=213&text=СЕО%20оптимизация%20сайта'), client.scrapyGet(url='https://yandex.ru/search/?text=СЕО%20Сайта&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=СЕО%20продвижение&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=СЕО%20оптимизация%20и%20продвижение&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=Анализ%20сайта%20на%20seo&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=Seo%20анализ%20сайта%20&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=Анализ%20сайта%20на%20seo%20оптимизацию%20онлайн&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=Анализ%20сайта%20онлайн%20бесплатно%20seo&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=проверка%20сайта%20на%20ссылки&lr=213'), client.scrapyGet(url='https://www.google.com/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+seo&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=Seo+%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+seo+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8E+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD+%D0%B1%D0%B5%D1%81%D0%BF%D0%BB%D0%B0%D1%82%D0%BD%D0%BE+seo&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D1%81%D0%B0%D0%B9%D1%82+url+%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+%D1%81%D0%B5%D0%BE&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD+seo&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%B2%D0%B2%D0%B5%D0%B4%D0%B8%D1%82%D0%B5+%D0%B4%D0%BE%D0%BC%D0%B5%D0%BD+%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D1%82%D1%8C+seo&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=SEO-%D0%B0%D1%83%D0%B4%D0%B8%D1%82+%D0%B1%D0%B5%D1%81%D0%BF%D0%BB%D0%B0%D1%82%D0%BD%D0%BE+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0+%D0%98%D0%9A%D0%A1+%D0%BF%D0%BE%D0%B7%D0%B8%D1%86%D0%B8%D0%B9+seo&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%B2%D0%B2%D0%B5%D0%B4%D0%B8%D1%82%D0%B5+%D0%B0%D0%B4%D1%80%D0%B5%D1%81%D0%B0+%D1%81%D0%B0%D0%B9%D1%82+%D0%B4%D0%BB%D1%8F+%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B8+%D0%BD%D0%B0+seo+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8E&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+%D1%81%D1%81%D1%8B%D0%BB%D0%BA%D0%B8&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8'), client.scrapyGet(url='https://www.google.com/search?q=%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%B8%D1%82%D1%8C+%D1%81%D0%B0%D0%B9%D1%82+%D0%BD%D0%B0+seo+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8E&rlz=1C9BKJA_enRU892RU892&hl=en-GB&sourceid=chrome-mobile&ie=UTF-8#'), client.scrapyGet(url='https://go.mail.ru/search?fr=main&us=33&usln=0&usstr=проверка%20сайта%20на%20seo%20оптимизацию&usqid=b4e7ef53a047dc50&sbmt=1590927369204&src=go&q=проверка%20сайта%20на%20seo%20оптимизацию&hasnavig=0'), client.scrapyGet(url='https://go.mail.ru/search?q=проверка%20сайта%20на%20ссылки&sbmt=1590927241137&src=go&frm=main&fr=main'), client.scrapyGet(url='https://go.mail.ru/search?fr=main&q=проверка%20ИКС%20позиций%20seo&sbmt=1590927085847&src=go'), client.scrapyGet(url='https://go.mail.ru/search?q=введите%20домен%20анализировать%20seo&sbmt=1590927169564&src=go&frm=main&fr=main'), client.scrapyGet(url='https://go.mail.ru/search?q=Анализ%20сайта%20онлайн%20seo&sbmt=1590927003173&src=go&frm=main&fr=main'), client.scrapyGet(url='https://go.mail.ru/search?q=анализ+сайта+на+сео&fm=1'), client.scrapyGet(url='https://go.mail.ru/search?q=сайт+url+анализ&fm=1'), client.scrapyGet(url='https://go.mail.ru/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD+%D0%B1%D0%B5%D1%81%D0%BF%D0%BB%D0%B0%D1%82%D0%BD%D0%BE+seo&fm=1'), client.scrapyGet(url='https://go.mail.ru/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+seo+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8E+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&fm=1'), client.scrapyGet(url='https://go.mail.ru/search?q=Seo+%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+&fm=1'), client.scrapyGet(url='https://go.mail.ru/search?q=%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%BD%D0%B0+seo&fm=1'),]\n",
    "    yandex_engine='https://yandex.ru'\n",
    "    mail_engine='https://go.mail.ru'\n",
    "    google_engine='https://www.google.com'\n",
    "    #keys = ['СЕО оптимизация сайта', 'СЕО Сайта', 'СЕО продвижение', 'СЕО оптимизация и продвижение']\n",
    "    #surl = search_engine\n",
    "    #for key in keys: surl = surl + '%22' + key + '%22%3B%20'\n",
    "    \n",
    "    #start_urls.append(client.scrapyGet(url=surl))\n",
    "    url_data = np.array([])\n",
    "    url_counter = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        links =\\\n",
    "        response.css('div.organic__path a::attr(href)').getall()#xpath('@href').getall()\n",
    "        #print 'here we go'\n",
    "        #for link in links:\n",
    "        self.url_data = np.append(self.url_data, np.array(links))\n",
    "        self.url_counter+=len(links)     \n",
    "        next_page = response.css('a.pager__item_kind_next::attr(href)').get()\n",
    "        google = False\n",
    "        if next_page is None:\n",
    "            next_page = response.css('a.G0iuSb::attr(href)').get()\n",
    "            google=True\n",
    "        \n",
    "        if (self.url_counter<250000) and (next_page is not None):\n",
    "            if not google:\n",
    "                yield scrapy.Request(self.client.scrapyGet(url = self.yandex_engine+next_page), self.parse)\n",
    "            else:\n",
    "                yield scrapy.Request(self.client.scrapyGet(url = self.google_engine+next_page), self.parse)\n",
    "        \n",
    "        #links = response.css('b.organic__extralinks')\n",
    "        #self.url_data = np.append(self.url_data, np.array(links))\n",
    "        #self.url_counter+=len(links)\n",
    "        #if self.url_counter < 1000:\n",
    "        #next _page = response.css('')\n",
    "        #yield response.follow(next_page, self.parse)\n",
    "    def closed(self, reason):\n",
    "        print(self.url_counter)\n",
    "        raw = pd.DataFrame(data=self.url_data)\n",
    "        raw.to_csv(r'/home/jovyan/raw_url.csv', index = False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash \n",
    "mv yandex_spider.py scrapers/yandex/yandex_scraper/yandex_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd yandex_scraper/\n",
    "scrapy crawl yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('raw_url.csv', names=['URLs'], index_col=False)\n",
    "duplicates_off = data.drop_duplicates()\n",
    "duplicates_off.count()\n",
    "duplicates_off.to_csv(r'/home/jovyan/uniq_urls.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 3.5 MB/s eta 0:00:01    |██████████▊                     | 6.7 MB 3.5 MB/s eta 0:00:04     |███████████████████████████▉    | 17.5 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.18.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['hahaha', 'hello_world'], dtype='<U11')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "vlower = np.vectorize(str.lower)\n",
    "string = ['hahaHa', 'heLlo_WorlD']\n",
    "vlower(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second lvl crawler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "scrapy startproject csv_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for csv_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'csv_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['csv_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'csv_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Safari/605.1.15'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'csv_scraper.middlewares.CsvScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'csv_scraper.middlewares.CsvScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'csv_scraper.pipelines.CsvScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/yandex/csv_scraper/csv_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile csv_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "#from scraper_api import ScraperAPIClient\n",
    "from numpy import genfromtxt\n",
    "\n",
    "feed = genfromtxt('/home/jovyan/uniq_urls.csv', dtype='U1024', delimiter=',')\n",
    "print(feed[0])\n",
    "\n",
    "class CSVSpider(scrapy.Spider):\n",
    "    name = 'csv'\n",
    "    start_urls = feed.tolist()#['http://yabs.yandex.ru/count/WWiejI_zO9C13H0091j-_5acsRmKqWK0am8GW0Wn9XRJNW00000u109myAxKiWU00UE8gTp8Wlt1OOW1oxYJYqQG0QIhWSqsc07Io-MsFRW1rfkEzHt00LpO0RxIjnJe0NIW0iAIeXRO0WJm0hsCuEa8c0E_vZkxBlW4lewK0OW5lewK0P05pz6z1Q05-PSjg0NwqoMm1VhJ9RW5-h2o0S05-z7k4iW5aFBbpeh23A06oie3g0RAoWF91YOEQxHy_Dk8gGTZOdZk3i-5AR07W82OFBW7j0Rn1pFlAYGNypCLX8A0WOI1eOcH2yaA-4k_n1HIxFWAWBKOgWiG_JujeMDY001X6BfiwAO50DaBw0k-ZfG1y0iAY0oenTw-0QaCL3O9LL0apR_e32FW3OE0W4293dIr5VlBxkkcdAoZou__xTWE1Q4Fl7IJ60HMy3_P3-0F0O0GYwgU4GZG4D-TrcWVxedRUJ-n4e-vf1DhtEW3a1Cou1FwiB81e1JwiB81g1JFqRq5w1G8o1M1hE_1cm7O5S6AzkoZZxpyO_2W5j3CmFO5oHRG5gZ5thu1WHUO5wsVeG-e5m705mJO5y24FHS0DCbfBCY9orPozFJyz44SUt98014nuVQCmGA3plHAkOx38zKMPZbo3q2qDoQuj5n8S38OfSra1F7ld0jjpYbsh7fmJqdhrpnQZH1Z~1?from=yandex.ru%3Bsearch%26%23x2F%3B%3Bweb%3B%3B0%3B&q=%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5+%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D0%B8+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5&etext=2202.avvFLkzvSQ-tvX01KexcMz-VBs4ZyDSu0Xo5NGkMb319fZaRio0h4DD35V_zWB2H1chh5bmhUTo03tOihoUB_PL-ahpEyc2Zrw8UgkDvl-3IDgWZ1g0gHPfixBMFLf7RdyVNxgnLe4LEGKNJRLFI1tmJ5lS1jJ101hC9JCdRoR5TPE-LVfcpEplQMXBvsBhVoqqUZApVJoYv3DwJdFtunHdsam9kZHhkcWl4aXRlaHY.28cd57c29b34cb6434c757edf4fa6ba66026f34d',]\n",
    "    #start_urls.append(client.scrapyGet(url=surl))\n",
    "    \n",
    "    #<div class=\"input\">\n",
    "          #<input type=\"text\" name=\"email\" placeholder=\"Ваш e-mail*\" data-obligatory=\"1\">\n",
    "          #<div class=\"error\"></div>\n",
    "            \n",
    "    #<div class=\"input-group review\">\n",
    "                    #<input type=\"text\" autocomplete=\"off\" spellcheck=\"false\" class=\"form-control\" placeholder=\"Введите адрес сайта\" name=\"url\" />\n",
    "                    #<span class=\"input-group-btn\"> <button class=\"btn btn-green\" type=\"submit\" id=\"review-btn\"><span class=\"glyphicon glyphicon-search\"></span> Обзор</button></span>\n",
    "                  #</div>\n",
    "            \n",
    "    #<div class=\"input-group review\">\n",
    "                    #<input type=\"text\" autocomplete=\"off\" spellcheck=\"false\" class=\"form-control\" placeholder=\"Введите адрес  сайта\" name=\"url\" />\n",
    "                    #<span class=\"input-group-btn\"> <button class=\"btn btn-green\" type=\"submit\" id=\"review-btn\"><span class=\"glyphicon glyphicon-search\"></span> ОБЗОР</button></span>\n",
    "                  #</div>\n",
    "            \n",
    "    url_data = np.array([])\n",
    "    url_counter = 0\n",
    "    \n",
    "    #def parse(self, response):\n",
    "        #string = response.url\n",
    "        #cake = re.search('.*\\..*?/', string)\n",
    "        #yield scrapy.Request(cake.group(0), self.purify)\n",
    "        \n",
    "    def parse(self, response):\n",
    "        placeholders = response.css('input[name ~= url]::attr(placeholder)').get()\n",
    "        if placeholders:\n",
    "            self.url_data = np.append(self.url_data,response.url)\n",
    "            self.url_counter+=1\n",
    "            placeholders = 0\n",
    "    \n",
    "    def closed(self, reason):\n",
    "        print('I am a little spudy |\\(o0vv0o)/|. Just ate ', self.url_counter, ' links, hehehe :3')\n",
    "        vlower = np.vectorize(str.lower)\n",
    "        self.url_data = vlower(self.url_data)\n",
    "        spudy = pd.DataFrame(data=self.url_data)\n",
    "        spudy.to_csv(r'/home/jovyan/spudy_url.csv', index = False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv csv_spider.py scrapers/yandex/csv_scraper/csv_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd csv_scraper/\n",
    "scrapy crawl csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spudy_url.csv', names=['URLs'], index_col=False)\n",
    "duplicates_off = data.drop_duplicates()\n",
    "duplicates_off.count()\n",
    "duplicates_off.head()\n",
    "duplicates_off.to_csv(r'/home/jovyan/pure_urls.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas\n",
    "import psycopg2\n",
    "from contextlib import closing\n",
    "\n",
    "with closing(psycopg2.connect(dbname='ruby.db.elephantsql.com (ruby-01)', user='rdqotkgq', password='xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S', host='postgres://rdqotkgq:xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S@ruby.db.elephantsql.com:5432/rdqotkgq')) as conn:\n",
    "    print(\"ama turtle, hello!\")\n",
    "    #with conn.cursor() as cursor:\n",
    "        #cursor.execute('SELECT * FROM airport LIMIT 5')\n",
    "        #for row in cursor:\n",
    "            #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\"SEO анализ\" | \"СЕО анализ\" | \"анализ\" | \"бесплатный анализ\" | \"Бесплатный СЕО анализ\" | \"Бесплатный SEO анализ\") + \"сайта\" + \" онлайн\"\n",
    "SEO бесплатный анализ сайта онлайн \"ввести\" \"url\" \n",
    "# remember to install the library: pip install scraperapi-sdk\n",
    "#from scraper_api import ScraperAPIClient\n",
    "#client = ScraperAPIClient('YOURAPIKEY')\n",
    "#result = client.get(url = 'http://httpbin.org/ip').text\n",
    "#print(result);\n",
    "# Scrapy users can simply replace the urls in their start_urls and parse function\n",
    "# Note for Scrapy, you should not use DOWNLOAD_DELAY and\n",
    "# RANDOMIZE_DOWNLOAD_DELAY, these will lower your concurrency and are not\n",
    "# needed with our API\n",
    "\n",
    "# ...other scrapy setup code\n",
    "#start_urls =[client.scrapyGet(url = 'http://httpbin.org/ip')]\n",
    "#def parse(self, response):\n",
    "\n",
    "# ...your parsing logic here\n",
    "#yield scrapy.Request(client.scrapyGet(url = 'http://httpbin.org/ip'), self.parse)\n",
    "\n",
    "#СЕО оптимизация сайта СЕО Сайта СЕО продвижение СЕО оптимизация и продвижение\n",
    "#'https://yandex.com/?q=%22СЕО оптимизация сайта%22%3B%20%22СЕО Сайта%22%3B%20%22СЕО продвижение%22%3B%20%22СЕО оптимизация и продвижение%22%3B%20'\n",
    "# organic__extralinks\n",
    "\n",
    "Анализ сайта на seo\n",
    "Seo анализ сайта \n",
    "Анализ сайта на seo оптимизацию онлайн\n",
    "Анализ сайта онлайн бесплатно seo\n",
    "\n",
    "\"\"\"\n",
    "here are more examples:\n",
    "https://24ho.ru/\n",
    "https://seo.analizsaita.online/\n",
    "https://pr-cy.ru/analysis/\n",
    "https://sitechecker.pro/ru/\n",
    "https://zen.yandex.ru/\n",
    "\n",
    "потом добавить туда \"проверка сайта на ссылки\"\n",
    "\n",
    "#фразы\n",
    "Анализ сайта на seo\n",
    "Seo анализ сайта \n",
    "Анализ сайта на seo оптимизацию онлайн\n",
    "Анализ сайта онлайн бесплатно seo\n",
    "сайт url анализ\n",
    "анализ сайта на сео\n",
    "Анализ сайта онлайн seo\n",
    "введите домен анализировать seo\n",
    "SEO-аудит бесплатно онлайн\n",
    "проверка ИКС позиций seo\n",
    "введите адреса сайт для проверки на seo оптимизацию\n",
    "проверка сайта на ссылки\n",
    "проверить сайт на seo оптимизацию\n",
    "\n",
    "\"\"\"\n",
    "\n",
    " <td aria-level=\"3\" class=\"b d6cvqb\" role=\"heading\"><a class=\"G0iuSb\" href=\"/search?q=fam+check&amp;biw=1173&amp;bih=837&amp;sxsrf=ALeKk02ohiot3BqFTlcP2zHsFflodIUmAQ:1590948934732&amp;ei=RvTTXs-XLOHzqwHj6IiYBA&amp;start=20&amp;sa=N&amp;ved=2ahUKEwjP0vDQ2t7pAhXh-SoKHWM0AkM4ChDw0wN6BAgLEEI\" id=\"pnnext\" style=\"text-align:left\"><span class=\"SJajHc NVbCr\" style=\"background:url(/images/nav_logo299.png) no-repeat;background-position:-96px 0;width:71px\"></span><span style=\"display:block;margin-left:53px\">Следующая</span></a></td>\n",
    "\n",
    "#start_urls = [client.scrapyGet(url = 'yandex.ru/search/?text=%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%A1%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22&rdrnd=662718&lr=213&redircnt=1590621126.1'), ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
