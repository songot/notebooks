{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mkdir scrapers/\n",
    "mkdir scrapers/yandex\n",
    "cd scrapers/yandex\n",
    "python -m pip install virtualenv\n",
    "virtualenv env\n",
    ". env/bin/activate\n",
    "python -m pip install scrapy\n",
    "python -m pip install numpy\n",
    "python -m pip install pandas\n",
    "python -m pip install requests\n",
    "python -m pip install scraperapi-sdk\n",
    "scrapy startproject yandex_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for yandex_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'yandex_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['yandex_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'yandex_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'yandex_scraper (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'yandex_scraper.middlewares.YandexScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'yandex_scraper.pipelines.YandexScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/yandex/yandex_scraper/yandex_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile yandex_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scraper_api import ScraperAPIClient\n",
    "\n",
    "class YandexSpider(scrapy.Spider):\n",
    "    name = 'yandex'\n",
    "    client = ScraperAPIClient('20195ff71f205bc83477c2cc97a76379')\n",
    "    start_urls = [client.scrapyGet(url='https://yandex.ru/search/?text=СЕО%20оптимизация%20сайта%20СЕО%20Сайта%20СЕО%20продвижение%20СЕО%20оптимизация%20и%20продвижение&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=бесплатный%20анализ%20сайта%20%22Введите%22%20%22url%22&lr=213'), client.scrapyGet(url='https://yandex.ru/search/?text=сайт%20url%20анализ&lr=213'),]\n",
    "    search_engine='https://yandex.ru'\n",
    "    #keys = ['СЕО оптимизация сайта', 'СЕО Сайта', 'СЕО продвижение', 'СЕО оптимизация и продвижение']\n",
    "    #surl = search_engine\n",
    "    #for key in keys: surl = surl + '%22' + key + '%22%3B%20'\n",
    "    \n",
    "    #start_urls.append(client.scrapyGet(url=surl))\n",
    "    url_data = np.array([])\n",
    "    url_counter = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        links =\\\n",
    "        response.css('div.organic__path a::attr(href)').getall()#xpath('@href').getall()\n",
    "        #print 'here we go'\n",
    "        for link in links:\n",
    "            self.url_data = np.append(self.url_data, link)\n",
    "            self.url_counter+=1\n",
    "        next_page = response.css('a.pager__item_kind_next::attr(href)').get()\n",
    "        if (self.url_counter<150000) and (next_page is not None):\n",
    "            yield scrapy.Request(self.client.scrapyGet(url = self.search_engine+next_page), self.parse)\n",
    "        \n",
    "        #links = response.css('b.organic__extralinks')\n",
    "        #self.url_data = np.append(self.url_data, np.array(links))\n",
    "        #self.url_counter+=len(links)\n",
    "        #if self.url_counter < 1000:\n",
    "        #next _page = response.css('')\n",
    "        #yield response.follow(next_page, self.parse)\n",
    "    def closed(self, reason):\n",
    "        print(self.url_counter)\n",
    "        raw = pd.DataFrame(data=self.url_data)\n",
    "        raw.to_csv(r'/home/jovyan/raw_url.csv', index = False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash \n",
    "mv yandex_spider.py scrapers/yandex/yandex_scraper/yandex_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd yandex_scraper/\n",
    "scrapy crawl yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('raw_url.csv', names=['URLs'], index_col=False)\n",
    "duplicates_off = data.drop_duplicates()\n",
    "duplicates_off.count()\n",
    "duplicates_off.to_csv(r'/home/jovyan/uniq_urls.csv', index = False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second lvl crawler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "scrapy startproject csv_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile settings.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for csv_scraper project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'csv_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['csv_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'csv_scraper.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'csv_scraper (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'csv_scraper.middlewares.CsvScraperSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'csv_scraper.middlewares.CsvScraperDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'csv_scraper.pipelines.CsvScraperPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv settings.py scrapers/yandex/csv_scraper/csv_scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing csv_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile csv_spider.py\n",
    "import scrapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scraper_api import ScraperAPIClient\n",
    "\n",
    "class CSVSpider(scrapy.Spider):\n",
    "    name = 'csv'\n",
    "    start_urls = ['http://yabs.yandex.ru/count/WWiejI_zO9C13H0091j-_5acsRmKqWK0am8GW0Wn9XRJNW00000u109myAxKiWU00UE8gTp8Wlt1OOW1oxYJYqQG0QIhWSqsc07Io-MsFRW1rfkEzHt00LpO0RxIjnJe0NIW0iAIeXRO0WJm0hsCuEa8c0E_vZkxBlW4lewK0OW5lewK0P05pz6z1Q05-PSjg0NwqoMm1VhJ9RW5-h2o0S05-z7k4iW5aFBbpeh23A06oie3g0RAoWF91YOEQxHy_Dk8gGTZOdZk3i-5AR07W82OFBW7j0Rn1pFlAYGNypCLX8A0WOI1eOcH2yaA-4k_n1HIxFWAWBKOgWiG_JujeMDY001X6BfiwAO50DaBw0k-ZfG1y0iAY0oenTw-0QaCL3O9LL0apR_e32FW3OE0W4293dIr5VlBxkkcdAoZou__xTWE1Q4Fl7IJ60HMy3_P3-0F0O0GYwgU4GZG4D-TrcWVxedRUJ-n4e-vf1DhtEW3a1Cou1FwiB81e1JwiB81g1JFqRq5w1G8o1M1hE_1cm7O5S6AzkoZZxpyO_2W5j3CmFO5oHRG5gZ5thu1WHUO5wsVeG-e5m705mJO5y24FHS0DCbfBCY9orPozFJyz44SUt98014nuVQCmGA3plHAkOx38zKMPZbo3q2qDoQuj5n8S38OfSra1F7ld0jjpYbsh7fmJqdhrpnQZH1Z~1?from=yandex.ru%3Bsearch%26%23x2F%3B%3Bweb%3B%3B0%3B&q=%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5+%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D0%B8+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5&etext=2202.avvFLkzvSQ-tvX01KexcMz-VBs4ZyDSu0Xo5NGkMb319fZaRio0h4DD35V_zWB2H1chh5bmhUTo03tOihoUB_PL-ahpEyc2Zrw8UgkDvl-3IDgWZ1g0gHPfixBMFLf7RdyVNxgnLe4LEGKNJRLFI1tmJ5lS1jJ101hC9JCdRoR5TPE-LVfcpEplQMXBvsBhVoqqUZApVJoYv3DwJdFtunHdsam9kZHhkcWl4aXRlaHY.28cd57c29b34cb6434c757edf4fa6ba66026f34d',]\n",
    "    #start_urls.append(client.scrapyGet(url=surl))\n",
    "    \n",
    "    #<div class=\"input\">\n",
    "          #<input type=\"text\" name=\"email\" placeholder=\"Ваш e-mail*\" data-obligatory=\"1\">\n",
    "          #<div class=\"error\"></div>\n",
    "            \n",
    "    #<div class=\"input-group review\">\n",
    "                    #<input type=\"text\" autocomplete=\"off\" spellcheck=\"false\" class=\"form-control\" placeholder=\"Введите адрес сайта\" name=\"url\" />\n",
    "                    #<span class=\"input-group-btn\"> <button class=\"btn btn-green\" type=\"submit\" id=\"review-btn\"><span class=\"glyphicon glyphicon-search\"></span> Обзор</button></span>\n",
    "                  #</div>\n",
    "            \n",
    "    #<div class=\"input-group review\">\n",
    "                    #<input type=\"text\" autocomplete=\"off\" spellcheck=\"false\" class=\"form-control\" placeholder=\"Введите адрес  сайта\" name=\"url\" />\n",
    "                    #<span class=\"input-group-btn\"> <button class=\"btn btn-green\" type=\"submit\" id=\"review-btn\"><span class=\"glyphicon glyphicon-search\"></span> ОБЗОР</button></span>\n",
    "                  #</div>\n",
    "            \n",
    "    url_data = np.array([])\n",
    "    url_counter = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        placeholders = response.css('input[name ~= url]::attr(placeholder)').getall()\n",
    "        if placeholders:\n",
    "            self.url_data = np.append(self.url_data,response.url)\n",
    "            self.url_counter+=1\n",
    "    \n",
    "    def closed(self, reason):\n",
    "        print('I am little spudy |\\(o0vv0o)/|. Just ate ', self.url_counter, ' links, hehehe :3')\n",
    "        spudy = pd.DataFrame(data=self.url_data)\n",
    "        spudy.to_csv(r'/home/jovyan/spudy_url.csv', index = False, header=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "mv csv_spider.py scrapers/yandex/csv_scraper/csv_scraper/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ваш e-mail *', 'Ваш e-mail*', 'Ваш e-mail', 'Ваш e-mail*', 'Ваш e-mail*']\n",
      "https://internet-clients.com/uslugi/prodvizhenie_2/?utm_source=direct&utm_medium=cpc&utm_campaign=36148491&utm_content=8244736745&utm_term=%D1%81%D0%B5%D0%BE%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%81%D0%B0%D0%B9%D1%82%D0%BE%D0%B2&yclid=2994991466454482722\n",
      "I am little spudy (**vv**)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 05:36:25 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: csv_scraper)\n",
      "2020-05-31 05:36:25 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.14.138+-x86_64-with-debian-buster-sid\n",
      "2020-05-31 05:36:25 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-05-31 05:36:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'csv_scraper',\n",
      " 'NEWSPIDER_MODULE': 'csv_scraper.spiders',\n",
      " 'SPIDER_MODULES': ['csv_scraper.spiders']}\n",
      "2020-05-31 05:36:25 [scrapy.extensions.telnet] INFO: Telnet Password: 4b2be25a3a7aa346\n",
      "2020-05-31 05:36:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-05-31 05:36:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-05-31 05:36:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-05-31 05:36:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-05-31 05:36:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-05-31 05:36:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-05-31 05:36:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-05-31 05:36:25 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to <GET https://internet-clients.com/uslugi/prodvizhenie_2/?utm_source=direct&utm_medium=cpc&utm_campaign=36148491&utm_content=8244736745&utm_term=%D1%81%D0%B5%D0%BE%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%81%D0%B0%D0%B9%D1%82%D0%BE%D0%B2&yclid=2994991466454482722> from <GET http://yabs.yandex.ru/count/WWiejI_zO9C13H0091j-_5acsRmKqWK0am8GW0Wn9XRJNW00000u109myAxKiWU00UE8gTp8Wlt1OOW1oxYJYqQG0QIhWSqsc07Io-MsFRW1rfkEzHt00LpO0RxIjnJe0NIW0iAIeXRO0WJm0hsCuEa8c0E_vZkxBlW4lewK0OW5lewK0P05pz6z1Q05-PSjg0NwqoMm1VhJ9RW5-h2o0S05-z7k4iW5aFBbpeh23A06oie3g0RAoWF91YOEQxHy_Dk8gGTZOdZk3i-5AR07W82OFBW7j0Rn1pFlAYGNypCLX8A0WOI1eOcH2yaA-4k_n1HIxFWAWBKOgWiG_JujeMDY001X6BfiwAO50DaBw0k-ZfG1y0iAY0oenTw-0QaCL3O9LL0apR_e32FW3OE0W4293dIr5VlBxkkcdAoZou__xTWE1Q4Fl7IJ60HMy3_P3-0F0O0GYwgU4GZG4D-TrcWVxedRUJ-n4e-vf1DhtEW3a1Cou1FwiB81e1JwiB81g1JFqRq5w1G8o1M1hE_1cm7O5S6AzkoZZxpyO_2W5j3CmFO5oHRG5gZ5thu1WHUO5wsVeG-e5m705mJO5y24FHS0DCbfBCY9orPozFJyz44SUt98014nuVQCmGA3plHAkOx38zKMPZbo3q2qDoQuj5n8S38OfSra1F7ld0jjpYbsh7fmJqdhrpnQZH1Z~1?from=yandex.ru%3Bsearch%26%23x2F%3B%3Bweb%3B%3B0%3B&q=%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D1%81%D0%B5%D0%BE+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5+%D1%81%D0%B5%D0%BE+%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F+%D0%B8+%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5&etext=2202.avvFLkzvSQ-tvX01KexcMz-VBs4ZyDSu0Xo5NGkMb319fZaRio0h4DD35V_zWB2H1chh5bmhUTo03tOihoUB_PL-ahpEyc2Zrw8UgkDvl-3IDgWZ1g0gHPfixBMFLf7RdyVNxgnLe4LEGKNJRLFI1tmJ5lS1jJ101hC9JCdRoR5TPE-LVfcpEplQMXBvsBhVoqqUZApVJoYv3DwJdFtunHdsam9kZHhkcWl4aXRlaHY.28cd57c29b34cb6434c757edf4fa6ba66026f34d>\n",
      "2020-05-31 05:36:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://internet-clients.com/uslugi/prodvizhenie_2/?utm_source=direct&utm_medium=cpc&utm_campaign=36148491&utm_content=8244736745&utm_term=%D1%81%D0%B5%D0%BE%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%81%D0%B0%D0%B9%D1%82%D0%BE%D0%B2&yclid=2994991466454482722> (referer: None)\n",
      "2020-05-31 05:36:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-05-31 05:36:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2237,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 18782,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 1.124217,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 5, 31, 5, 36, 26, 630358),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 55570432,\n",
      " 'memusage/startup': 55570432,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2020, 5, 31, 5, 36, 25, 506141)}\n",
      "2020-05-31 05:36:26 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "cd scrapers/yandex\n",
    ". env/bin/activate\n",
    "cd csv_scraper/\n",
    "scrapy crawl csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas\n",
    "import psycopg2\n",
    "from contextlib import closing\n",
    "\n",
    "with closing(psycopg2.connect(dbname='ruby.db.elephantsql.com (ruby-01)', user='rdqotkgq', password='xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S', host='postgres://rdqotkgq:xoQNG5GVJm5Xe0iyz7JjwFC2Ta8unK_S@ruby.db.elephantsql.com:5432/rdqotkgq')) as conn:\n",
    "    print(\"ama turtle, hello!\")\n",
    "    #with conn.cursor() as cursor:\n",
    "        #cursor.execute('SELECT * FROM airport LIMIT 5')\n",
    "        #for row in cursor:\n",
    "            #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(\"SEO анализ\" | \"СЕО анализ\" | \"анализ\" | \"бесплатный анализ\" | \"Бесплатный СЕО анализ\" | \"Бесплатный SEO анализ\") + \"сайта\" + \" онлайн\"\n",
    "SEO бесплатный анализ сайта онлайн \"ввести\" \"url\" \n",
    "# remember to install the library: pip install scraperapi-sdk\n",
    "#from scraper_api import ScraperAPIClient\n",
    "#client = ScraperAPIClient('YOURAPIKEY')\n",
    "#result = client.get(url = 'http://httpbin.org/ip').text\n",
    "#print(result);\n",
    "# Scrapy users can simply replace the urls in their start_urls and parse function\n",
    "# Note for Scrapy, you should not use DOWNLOAD_DELAY and\n",
    "# RANDOMIZE_DOWNLOAD_DELAY, these will lower your concurrency and are not\n",
    "# needed with our API\n",
    "\n",
    "# ...other scrapy setup code\n",
    "#start_urls =[client.scrapyGet(url = 'http://httpbin.org/ip')]\n",
    "#def parse(self, response):\n",
    "\n",
    "# ...your parsing logic here\n",
    "#yield scrapy.Request(client.scrapyGet(url = 'http://httpbin.org/ip'), self.parse)\n",
    "\n",
    "#СЕО оптимизация сайта СЕО Сайта СЕО продвижение СЕО оптимизация и продвижение\n",
    "#'https://yandex.com/?q=%22СЕО оптимизация сайта%22%3B%20%22СЕО Сайта%22%3B%20%22СЕО продвижение%22%3B%20%22СЕО оптимизация и продвижение%22%3B%20'\n",
    "# organic__extralinks\n",
    "\n",
    "\"\"\"\n",
    "here are more examples:\n",
    "https://24ho.ru/\n",
    "https://seo.analizsaita.online/\n",
    "https://pr-cy.ru/analysis/\n",
    "https://sitechecker.pro/ru/\n",
    "https://zen.yandex.ru/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#start_urls = [client.scrapyGet(url = 'yandex.ru/search/?text=%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D1%81%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%A1%D0%B0%D0%B9%D1%82%D0%B0%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22%3B%20%22%D0%A1%D0%95%D0%9E%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%22&rdrnd=662718&lr=213&redircnt=1590621126.1'), ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
